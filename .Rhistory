nTables = nTables - 1
tablesValues[currentTable] = -1
}
nTablesInRestaurant = sum(tableRestaurantAllocation==indexRestaurant)
indecesTablesInRestaurant = (1:maxTableIndex)[tableRestaurantAllocation==indexRestaurant] # indeces of tables in the restaurant
indecesPossibleTables = (tablesValues[indecesTablesInRestaurant] == currentDish) # tables in the restaurant serving the current dish
possibleTables = c(indecesTablesInRestaurant[indecesPossibleTables],-1)
probNewTable = (theta + sigma*nTablesInRestaurant)/(nTables + theta0)
nTablesServingCurrentDish = sum(tablesValues == currentDish)
if(nTablesServingCurrentDish > 0) {
probNewTable = probNewTable*(nTablesServingCurrentDish - sigma0)
}
probs = c(nPeopleAtTable[indecesTablesInRestaurant][indecesPossibleTables] - sigma, probNewTable)
newTableAllocation = sample(possibleTables,1,replace = F, prob = probs)
if(newTableAllocation < 0) {
##### COMPUTE THE CONTRIBUTION PART OF THE LOG ACCEPTANCE PROBABILITY
individualsToRemove = c((firstIndividuals[indexGroup]-1+indexCustomerGroup):lastIndividuals[indexGroup])
nPeopleAtTableCS = as.integer(table(factor(observationTableAllocation[-individualsToRemove], levels = 1:maxTableIndex))) # nPeopleAtTable - as.integer(table(factor(observationTableAllocation[individualsToRemove], levels = 1:maxTableIndex))) # as.integer(table(factor(observationTableAllocation_noJ, levels = 1:maxTableIndex))) # this is wrong since it removes twice the current observation from the count of people at table
nTablesCS = sum(nPeopleAtTableCS>0)
# compute nTablesInRestaurantCS
# freeTablesCS = (1:maxTableIndex)[nPeopleAtTableCS == 0]
# tableRestaurantAllocationCS = tableRestaurantAllocation
# tableRestaurantAllocationCS[freeTablesCS] = -1
# nTablesInRestaurantCS = sum(tableRestaurantAllocationCS==indexRestaurant)
# nTablesInRestaurantCS =
#   length(table(observationTableAllocation[-individualsToRemove][observationRestaurantAllocation[-individualsToRemove]==indexRestaurant]))
#
# compute log of P[X_ji=x|____]
if(dishesCountsCS[currentDish] == 0){ # X_ji = "new"
logP_CS = logP_CS +
log(theta0 + nDishesCS*sigma0) -
log(theta0 + nTablesCS)
nDishesCS = nDishesCS + 1
} else{
# compute number of tables serving the current dish in the whole franchise, excluding individualsToRemove
# nTablesServingCurrentDishCS = length(table(observationTableAllocation[-individualsToRemove][observationDishAllocation[-individualsToRemove] == currentDish]))
# alternative computation:
freeTablesCS = (1:maxTableIndex)[nPeopleAtTableCS == 0] # which(nPeopleAtTableCS == 0)
tablesValuesCS = tablesValues
tablesValuesCS[freeTablesCS] = -1
nTablesServingCurrentDishCS = sum(tablesValuesCS==currentDish)
# get the tables in current restaurant serving the current dish (excluding future observations)
# freeTablesCS = (1:maxTableIndex)[nPeopleAtTableCS == 0] # which(nPeopleAtTableCS == 0)
# tableRestaurantAllocationCS = tableRestaurantAllocation
# tableRestaurantAllocationCS[freeTablesCS] = -1
# tablesInRestaurantServingCurrentDishCS = (1:maxTableIndex)[((tableRestaurantAllocationCS == indexRestaurant)&(tablesValues == currentDish))]
# tablesValuesCS = tablesValues
# tablesValuesCS[freeTablesCS] = -1
#nTablesInRestaurantServingCurrentDishCS = length(tablesInRestaurantServingCurrentDish)
logP_CS = logP_CS +
log(nTablesServingCurrentDishCS - sigma0) -
log(theta0 + nTablesCS)
}
### update quantities of "current state"
# nPeopleInRestaurantCS = nPeopleInRestaurantCS + 1
dishesCountsCS[currentDish] = dishesCountsCS[currentDish] + 1
##### UPDATE GIBBS SAMPLING QUANTITIES
nTables = nTables + 1
if(nFreeTables > 0) { # pick the first free table
newTableAllocation = freeTables[1]
freeTables = freeTables[-1]
nFreeTables = nFreeTables - 1
nPeopleAtTable[newTableAllocation] = 1
tablesValues[newTableAllocation] = currentDish
tableRestaurantAllocation[newTableAllocation] = indexRestaurant # assign table to restaurant
} else { # create a new table
maxTableIndex = maxTableIndex + 1
newTableAllocation = maxTableIndex
nPeopleAtTable = c(nPeopleAtTable,1)
tablesValues = c(tablesValues,currentDish)
tableRestaurantAllocation = c(tableRestaurantAllocation,indexRestaurant) # assign table to restaurant
}
} else{ # the sampled table is already occupied in the restaurant --> just update the relevant quantities
nPeopleAtTable[newTableAllocation] = nPeopleAtTable[newTableAllocation] + 1
}
observationTableAllocation[indexCustomerGlobal] = newTableAllocation
indexCustomerGlobal = indexCustomerGlobal + 1
}
#### MH STEP
# we begin the MH proposal sampling
nRestMH = nRest
maxRestIndexMH = maxRestIndex
groupRestaurantAllocationMH = groupRestaurantAllocation
nGroupsInRestaurantMH = nGroupsInRestaurant
nGroupsInRestaurantMH[indexRestaurant] = nGroupsInRestaurantMH[indexRestaurant] - 1
nFreeRestaurantsMH = nFreeRestaurants
freeRestaurantsMH = freeRestaurants
if(nGroupsInRestaurantMH[indexRestaurant] == 0) {
nRestMH = nRestMH - 1
nFreeRestaurantsMH = nFreeRestaurantsMH + 1
freeRestaurantsMH = c(indexRestaurant,freeRestaurantsMH)
}
#### PROPOSE A NEW RESTAURANT AND TABLE ALLOCATION
#### sample restaurant
nonEmptyRest = (1:maxRestIndexMH)[nGroupsInRestaurantMH>0] # restaurants with at least one group assigned
possibleRestaurants = c(nonEmptyRest,-1)
probs =c(nGroupsInRestaurantMH[nonEmptyRest] - alpha, gamma + alpha*nRestMH)
newRestaurantAllocation = sample(possibleRestaurants,1,replace = F, prob = probs)
if(newRestaurantAllocation < 0) {
nRestMH = nRestMH + 1
if(nFreeRestaurantsMH > 0) { # pick the first free restaurant
newRestaurantAllocation = freeRestaurantsMH[1]
freeRestaurantsMH = freeRestaurantsMH[-1]
nFreeRestaurantsMH = nFreeRestaurantsMH - 1
nGroupsInRestaurantMH[newRestaurantAllocation] = 1
groupRestaurantAllocationMH[indexGroup] = newRestaurantAllocation # assign group to restaurant
} else { # create a new restaurant
maxRestIndexMH = maxRestIndexMH + 1
newRestaurantAllocation = maxRestIndexMH
nGroupsInRestaurantMH = c(nGroupsInRestaurantMH,1)
groupRestaurantAllocationMH[indexGroup] = newRestaurantAllocation # assign group to restaurant
}
} else{ # the sampled restaurants contains already some groups --> just update the relevant quantities
groupRestaurantAllocationMH[indexGroup] = newRestaurantAllocation # assign group to restaurant
nGroupsInRestaurantMH[newRestaurantAllocation] = nGroupsInRestaurantMH[newRestaurantAllocation] + 1
}
#### sample table allocation
# initialize MH quantities excluding current group j
maxTableIndexMH = maxTableIndex
observationTableAllocationMH = observationTableAllocation
observationTableAllocationMH[individualsInCurrentGroup] = -1
observationRestaurantAllocationMH = observationRestaurantAllocation_noJ # computed above
observationDishAllocationMH = observationDishAllocation
observationDishAllocationMH[individualsInCurrentGroup] = -1
nPeopleAtTableMH = as.integer(table(factor(observationTableAllocation[-individualsInCurrentGroup], levels = 1:maxTableIndex))) # nPeopleAtTable - as.integer(table(factor(observationTableAllocation[individualsInCurrentGroup], levels = 1:maxTableIndex))) # as.integer(table(factor(observationTableAllocation_noJ, levels = 1:maxTableIndex)))
nTablesMH = sum(nPeopleAtTableMH>0)
freeTablesMH = (1:maxTableIndexMH)[nPeopleAtTableMH == 0]
nFreeTablesMH = length(freeTablesMH)
tablesValuesMH = tablesValues
tablesValuesMH[freeTablesMH] = -1
tableRestaurantAllocationMH = tableRestaurantAllocation
tableRestaurantAllocationMH[freeTablesMH] = -1
# nPeopleInRestaurantMH = nPeopleInRestaurant_noJ # computed above
dishesCountsMH = dishesCounts_noJ
nDishesMH = nDishes - sum(dishesCountsMH==0)
# # nTablesServingCurrentDish = sum(tablesValues_noJ == observationDishAllocation[indexCustomerGlobal])
##### end computation of quantities without group J
# #### UPDATE TABLE CONFIGURATION IN THE CURRENT GROUP (analogous to the sampling from the full conditionals)
logP_MH = 0 # log of P[X_ji=x|____ S'] to be used in MH
for (indexCustomerGroup in 1:nByGroup[indexGroup]) { # this loop should be joined with the loop for the full conditionals!!!!!
indecesTablesInRestaurant = (1:maxTableIndexMH)[tableRestaurantAllocationMH==newRestaurantAllocation]
currentDish = observationDishAllocation[indexCustomerGlobalMH] # get the current dish
observationRestaurantAllocationMH[indexCustomerGlobalMH] = newRestaurantAllocation
nTablesInRestaurant = sum(tableRestaurantAllocationMH==newRestaurantAllocation)
indecesPossibleTables = (tablesValuesMH[indecesTablesInRestaurant] == observationDishAllocation[indexCustomerGlobalMH])
possibleTables = c(indecesTablesInRestaurant[indecesPossibleTables],-1)
probNewTable = (theta + sigma*nTablesInRestaurant)/(nTablesMH + theta0)
nTablesServingCurrentDishMH = sum(tablesValuesMH == observationDishAllocation[indexCustomerGlobalMH])
if(nTablesServingCurrentDishMH > 0) {
probNewTable = probNewTable*(nTablesServingCurrentDishMH - sigma0)
}
probs = c(nPeopleAtTableMH[indecesTablesInRestaurant][indecesPossibleTables] - sigma, probNewTable)
newTableAllocation = sample(possibleTables,1,replace = F, prob = probs)
if(newTableAllocation < 0) {
# compute log of P[X_ji=x|____S']
if(dishesCountsMH[currentDish] == 0){ # X_ji = "new"
logP_MH = logP_MH +
log(theta0 + nDishesMH*sigma0) -
log(theta0 + nTablesMH)
} else{
# compute number of tables serving the current dish in the whole franchise, excluding future individuals
nTablesServingCurrentDishMH = length(table(observationTableAllocationMH[observationDishAllocationMH == currentDish]))
# get the tables in current restaurant serving the current dish (excluding future observations)
# tablesInRestaurantServingCurrentDish = (1:maxTableIndexMH)[((tableRestaurantAllocationMH == newRestaurantAllocation)&(tablesValuesMH == currentDish))]
logP_MH = logP_MH +
log(nTablesServingCurrentDishMH - sigma0) -
log(theta0 + nTablesMH)
}
### update quantities of MH
# nPeopleInRestaurantMH = nPeopleInRestaurantMH + 1
dishesCountsMH[currentDish] = dishesCountsMH[currentDish] + 1
observationDishAllocationMH[indexCustomerGlobalMH] = currentDish
nTablesMH = nTablesMH + 1
if(nFreeTablesMH > 0) { # pick the first free table
newTableAllocation = freeTablesMH[1]
freeTablesMH = freeTablesMH[-1]
nFreeTablesMH = nFreeTablesMH - 1
nPeopleAtTableMH[newTableAllocation] = 1
tablesValuesMH[newTableAllocation] = observationDishAllocationMH[indexCustomerGlobalMH]
tableRestaurantAllocationMH[newTableAllocation] = newRestaurantAllocation # assign table to restaurant
} else { # create a new table
maxTableIndexMH = maxTableIndexMH + 1
newTableAllocationMH = maxTableIndexMH
nPeopleAtTableMH = c(nPeopleAtTableMH,1)
tablesValuesMH = c(tablesValuesMH,observationDishAllocationMH[indexCustomerGlobalMH])
tableRestaurantAllocationMH = c(tableRestaurantAllocationMH,newRestaurantAllocation) # assign table to restaurant
}
} else{ # the sampled table is already occupied in the restaurant --> just update the relevant quantities
nPeopleAtTableMH[newTableAllocation] = nPeopleAtTableMH[newTableAllocation] + 1
}
observationTableAllocationMH[indexCustomerGlobalMH] = newTableAllocation
# update indexCustomerGlobalMH
indexCustomerGlobalMH = indexCustomerGlobalMH + 1
}
pAccept = min(exp(logP_MH - logP_CS),1)
accept = runif(1) < pAccept
# print(pAccept)
if(accept == T){
nRest = nRestMH
maxRestIndex = maxRestIndexMH
groupRestaurantAllocation[indexGroup] = groupRestaurantAllocationMH[indexGroup]
nGroupsInRestaurant = nGroupsInRestaurantMH
nFreeRestaurants = nFreeRestaurantsMH
freeRestaurants = freeRestaurantsMH
maxTableIndex = maxTableIndexMH
observationTableAllocation = observationTableAllocationMH
observationRestaurantAllocation = observationRestaurantAllocationMH
nPeopleAtTable = nPeopleAtTableMH
nTables = nTablesMH
freeTables = freeTablesMH
nFreeTables = nFreeTablesMH
tablesValues = tablesValuesMH
tableRestaurantAllocation = tableRestaurantAllocationMH
}
##### END OF METROPOLIS HASTINGS STEP
### EDIT THIS PART TO UPDATE THE TABLES ALLOCATIONS FOR THE WHOLE GROUP
groupRestaurantAllocationAcrossGibbs[r, indexGroup] = groupRestaurantAllocation[indexGroup]
observationTableAllocationAcrossGibbs[r,firstIndividuals[indexGroup]:lastIndividuals[indexGroup]] =
observationTableAllocation[firstIndividuals[indexGroup]:lastIndividuals[indexGroup]]
}
### SAMPLE TABLE AND DISH FOR OUT-OF-SAMPLE OBSERVATIONS
# THIS HAS BEEN MOVED OUT OF THE LOOP FOR THE MOMENT
}
nTablesServingCurrentDish
sigma0
log(nTablesServingCurrentDishCS - sigma0)
nTablesServingCurrentDishCS
g=0.1; p=0.9; (1+g)/p + (1-p)/p
g=0.001; p=0.999; (1+g)/p + (1-p)/p
60/16
60/76
10/11
10000/(40*60)
unlink("Library/CloudStorage/Dropbox/Teaching/UNITO/ISL/Rebaudo/Rebaudo - 2023:24/Lect 13-14 Cross Validation and Resampling upload/Chp 5 - Validation and Resampling_cache", recursive = TRUE)
library(ISLR2)
# Lab: Cross-Validation and the Bootstrap
library(ISLR2)
?library
set.seed(1)
?Auto
train <- sample(392,196)
train
lm.fit <- lm(mpg~horsepower, data=Auto, subset = train)
lm.fit
summary(lm.fit)
attach(Auto)
mpg
mean( (mpg-predict(lm.fit, Auto))[-train]^2 )
plot(mpg~horsepower)
lm.fit2 <- lm(mpg~poly(horsepower,2), data=Auto, subset = train)
mean( (mpg-predict(lm.fit2, Auto))[-train]^2 )
lm.fit3 <- lm(mpg~poly(horsepower,3), data=Auto, subset = train)
mean( (mpg-predict(lm.fit3, Auto))[-train]^2 )
set.seed(2)
train <- sample(392,196)
set.seed(2)
train <- sample(392,196)
lm.fit <- lm(mpg~horsepower, data=Auto, subset = train)
mean( (mpg-predict(lm.fit, Auto))[-train]^2 )
lm.fit2 <- lm(mpg~poly(horsepower,2), data=Auto, subset = train)
mean( (mpg-predict(lm.fit2, Auto))[-train]^2 )
lm.fit3 <- lm(mpg~poly(horsepower,3), data=Auto, subset = train)
mean( (mpg-predict(lm.fit3, Auto))[-train]^2 )
glm.fit(mpg~horsepower, data=Auto)
glm.fit(mpg~horsepower)
glm.fit <- glm(mpg~horsepower, data=Auto)
coef(glm.fit)
glm.fit <- glm(mpg~horsepower, data=Auto)
coef(glm.fit)
lm.fit <- lm(mpg~horsepower, data=Auto)
coef(lm.fit)
library(boot)
library(boot)
cv.err <- cv.glm(Auto, glm.fit)
cv.err
cv.err$delta
cv.error <- double(10)
cv.error
cv.error <- double(10)
for (i in 1:10){
glm.fit <- glm(mpg~poly(horsepower, i), data=Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
plot(cv.error,type="b")
loocv <- function(fit){
h <- lm.influence(fit)$h
mean((residual(fit)/(1-h))^2)
}
set.seed(17)
set.seed(17)
cv.error.10 <- rep(0, 10)
cv.error.10
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10){
glm.fit <- glm(mpg~poly(horsepower, i), data=Auto)
cv.error[i] <- cv.glm(Auto, glm.fit, K=10)$delta[1]
}
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10){
glm.fit <- glm(mpg~poly(horsepower, i), data=Auto)
cv.error.10[i] <- cv.glm(Auto, glm.fit, K=10)$delta[1]
}
cv.error.10
lines(cv.error.10, type="b", col=2)
i
i=1
glm.fit <- glm(mpg~poly(horsepower, i), data=Auto)
cv.glm(Auto, glm.fit, K=10)$delta
cv.glm(Auto, glm.fit)$delta
alpha.fn <- function(data, index){
X <- data$X[index]
Y <- data$Y[index]
(var(Y)-cov(X,Y))/(Var(X)+Var(Y)-2*cov(X,Y))
}
alpha.fn(Portfolio,1:100)
alpha.fn <- function(data, index){
X <- data$X[index]
Y <- data$Y[index]
(var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y))
}
alpha.fn(Portfolio,1:100)
alpha.fn(Portfolio, sample(100, 100, replace=T))
boot(Portfolio, alpha.fn, R=1000)
alpha.fn(Portfolio,1:100)
boot.fn <- function(data, index){}
boot.fn <- function(data, index){
coef(lm(mpg~horsepower, data=data, subset=index))
}
boot.fn(Auto, 1:392)
set.seed(1)
boot.fn(Auto, sample(392, 392, replace=T))
boot(Auto, boot.fn, 1000)
summary(lm(mpg~horsepower, data=Auto))$coef
94/4
23.5/4
library(ISLR2)
x <- model.matrix(Salary ~ ., Hitters)
x
x <- model.matrix(Salary ~ ., Hitters)[,-1]
x
y <- Hitters$Salary
y
Hitters <- na.omit(Hitters)
library(ISLR2)
library(leaps)
Hitters <- na.omit(Hitters)
regfit.bwd <- regsubsets(Salary ~., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
plot(regfit.bwd, scale = "cp")
plot(regfit.bwd, scale = "Cp")
library(glmnet)
x <- model.matrix(Salary~., Hitters)
x
x <- model.matrix(Salary~., Hitters)[,-1]
y <- Hitters$Salary
grid <- 10^seq(10,-2, length=100)
grid
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod))
plot(ridge.mod, xvar = "lambda", label = TRUE)
ridge.mod$lambda[50]
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50])^2)
sqrt(sum(coef(ridge.mod)[-1,100])^2)
ridge.mod$lambda[100]
predict(ridge.mod, s=50, type="coefficient")[1:20,]
set.seed(1)
cv.out <- cv.glmnet(x, y, alpha=0)
plot(cv.out)
set.seed(1)
train <- sample(1:nrow(x), nrow/2)
train <- sample(1:nrow(x), nrow(x)/2)
train
test <- (-train)
test
train
y.test <- y[test]
y.test
ridge.mod <- glmnet(x[train,], y[train], alpha=0, lambda = grid, thresh=1e-12)
ridge.mod <- predict(ridge.mod, s=4, newx = x[test,])
ridge.mod <- glmnet(x[train,], y[train], alpha=0, lambda = grid, thresh=1e-12)
ridge.pred <- predict(ridge.mod, s=4, newx = x[test,])
ridge.pred
mean((ridge.pred-y.test)^2)
cv.out <- cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
cv.out$lambda.min
bestlam <- cv.out$lambda.min
ridge.pred <- predict(ridge.mod, s=bestlam, newx = x[test,])
mean((ridge.pred-y.test)^2)
# Lasso
lasso.mod <- glmnet(x[train,], y[train], alpha=1, lambda = grid)
plot(lasso.mod)
plot(lasso.mod, xvar="lambda")
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
ridge.mod <- glmnet(x[train,], y[train], alpha=0, lambda = grid, thresh=1e-12)
cv.out <- cv.glmnet(x[train,], y[train], alpha=0)
bestlam <- cv.out$lambda.min
ridge.pred <- predict(ridge.mod, s=bestlam, newx = x[test,])
mean((ridge.pred-y.test)^2)
mean((lasso.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
# PCR
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary~., data=Hitters, scale=TRUE, validationplot="CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type="MSEP")
set.seed(1)
pls.fit <- plsr(Salary~., data=Hitters, subset=train, scale=T, validation="CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
library(updateR)
updateR()
2+2
library(ISLR2)
attach(Wage)
fit <- lm(wage ~ poly(age, 4), data=Wage)
coef(summary(fit))
fit2 <- lm(wage ~ poly(age, 4, raw=T), data=Wage)
coef(summary(fit2))
fit2a <- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data=Wage)
coef(summary(fit2a))
agelims <- range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])
preds <- predict(fit, newdata = list(age=age.grid), se=T)
se.bands <- cbind(preds$fit + 2*preds$se.fit,  preds$fit - 2*preds$se.fit)
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
lines(age.grid, preds$fit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col="blue", lty=3)
preds2 <- predict(fit2, newdata=list(age=age.grid), se=T)
plot(preds$fit, preds2$fit)
fit.1 <- lm(wage ~ age, data=Wage)
fit.2 <- lm(wage ~ poly(age,2), data=Wage)
fit.1 <- lm(wage ~ age, data=Wage)
fit.2 <- lm(wage ~ poly(age,2), data=Wage)
fit.3 <- lm(wage ~ poly(age,3), data=Wage)
fit.4 <- lm(wage ~ poly(age,4), data=Wage)
fit.5 <- lm(wage ~ poly(age,5), data=Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
fit <- glm(I(wage > 250) ~ poly(age, 4), data=Wage, family = binomial)
preds <-  predict(fit, newdata=list(age=age.grid), se=T)
preds
exp(preds$fit)/(1 + exp(preds$fit))
pfit <- exp(preds$fit)/(1 + exp(preds$fit))
pfit
se.bands.logit <- cbind(preds$fit + 2 *preds$se.fit, preds$fit - 2 * preds$se.fit)
se.bands <-  exp(se.bands.logit)/(1 + exp(se.bands.logit))
plot(age, I(wage > 250), xlim=agelims, type="n", ylim=c(0, 0.2))
points(jitter(age), I(wage > 250)/5, cex=.5, pch="|", col="darkgrey")
lines(age.grid, pfit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col="blue", lty=3)
fit <- lm(wage ~ cut(age,4), data=Wage)
table(cut(age,4))
coef(fit)
library(splines)
fit <-lm(wage~ bs(age, knots=c(25,40,60)), data=Wage)
pred <- predict(fit, newdata=list(age=age.grid), se=T)
plot(age, wage, col="grey")
lines(age.grid, pred$fit, lwd=2, col="blue")
lines(age.grid, pred$fit + 2 * pred$se , col="blue", lty="dashed")
lines(age.grid, pred$fit - 2 * pred$se , col="blue", lty="dashed")
bs(age, df=6, "knots")
attr(bs(age, df=6), "knots")
fit2 <- lm(wage~ns(age, df=4),)
fit2 <- lm(wage~ns(age, df=4),data=Wage)
pred2 <- predict(fit2, newdata = list(age=age.grid), se=T)
plot(age, wage, col="grey")
lines(age.grid, pred$fit2, lwd=2, col="blue")
fit2 <- lm(wage~ns(age, df=4),data=Wage)
pred2 <- predict(fit2, newdata = list(age=age.grid), se=T)
plot(age, wage, col="grey")
lines(age.grid, pred2$fit2, lwd=2, col="blue")
lines(age.grid, pred2$fit, lwd=2, col="blue")
fit2 <- lm(wage~ns(age, df=4),data=Wage)
pred2 <- predict(fit2, newdata = list(age=age.grid), se=T)
plot(age, wage, col="grey")
lines(age.grid, pred2$fit, lwd=2, col="blue")
plot(age, wage, col="grey")
lines(age.grid, pred$fit, lwd=2, col="blue")
fit <- smooth.spline(age, wage, df=16)
fit2 <- smooth.spline(age, wage, cv=T)
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
lines(fit, col="red", lwd=2)
lines(fit2, col="blue", lwd=2)
legend("topright", legend("16 DF", "6.8 DF"), col=c("red", "blue"), lty=1, lwd=2, cex=.8)
legend("topright", legend("16 DF", "6.8 DF"), col=c("red", "blue"), lty=1, lwd=2, cex=.8)
legend("topright", legend=c("16 DF", "6.8 DF"), col=c("red", "blue"),
lty=1, lwd=2, cex=.8)
gam1 <- lm(wage ~ ns(year, 4) + ns(age,5) + education, data=Wage)
library(gam)
gam2 <- gam(wage ~ s(year,4) + s(age, 5) +education, data=Wage)
par(mfrow=c(1,3))
plot(gam2, se=T, col="blue")
source("~/Library/CloudStorage/Dropbox/GitHub/MSSP/MAB_HPYP.R", echo=TRUE)
source("~/Library/CloudStorage/Dropbox/GitHub/MSSP/MAB_HPYP.R", echo=TRUE)
save(result_HPY_mean,     file="./Data-and-Results/result_HPY_mean.RData")
save(results_random_mean, file="./Data-and-Results/results_random_mean.RData")
save(results_oracle_mean, file="./Data-and-Results/results_oracle_mean.RData")
warnings()
warnings()
